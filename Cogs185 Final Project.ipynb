{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffe3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36925572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Data I/O\n",
    "data = open('input.txt', 'r').read()  # This should be a simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4e29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100  # Size of hidden layer of neurons\n",
    "seq_len = 25  # Number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "epochs = 20\n",
    "save_path = 'model.pth'\n",
    "op_seq_len = 200  # Length of the output sequence\n",
    "num_layers = 3\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc201d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove incompatible cuDNN versions from LD_LIBRARY_PATH\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.environ['LD_LIBRARY_PATH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132dc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size).to(device))\n",
    "\n",
    "# Instantiate model, loss function and optimizer\n",
    "rnn = RNN(vocab_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3ca5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of characters to a tensor\n",
    "def char_tensor(seq):\n",
    "    tensor = torch.tensor([char_to_ix[ch] for ch in seq], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab559ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs_cnt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07be74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Loss: 3.97747574\n",
      "----------------------------------------\n",
      "been nOnTTOOnoedhh oiwh.tatk.lgoegah El  Ihs iTOTO T ThOoon oWhTtyegeTlilw boinEnghhEgylaTnnTs eginhET T,ooTl.OoTatdwetsgyaBnTT Tn ' lno..ishskwaat stnnOlwhato.nTbOgETyT.shnsneWit ;OgnwA agnnBghahtOnoT\n",
      "----------------------------------------\n",
      "Epoch: 2 \t Loss: 3.97823547\n",
      "----------------------------------------\n",
      "TneBoboOOgga'e'Wal TW nOT OleswwraaihheeTegllyWs  .rli.  . OaTOOOkeSm.TlsOnalel.ywa peegT;aEeawilIhl abaeenesOgrOO wn.ya.It t.naTO O .lawTgkIOkrpr OiWw 'Tw  lOrl, snaOgsrW  'aeTlmlOWeOOTstOmElrOgO  aoe\n",
      "----------------------------------------\n",
      "Epoch: 3 \t Loss: 3.69692694\n",
      "----------------------------------------\n",
      "\n",
      "u ubYYYY,;;;;Y,;;;YY;Yb;Y;; YY;Y;;;,YYYl;;;Y;;,YYY'YYYY;;Y;;;Y;Y;YYYYYYY;Y;;YYYY;,;YYYYY;;YYYY;YY;YY;YY;Y;;Y;;;,;;;;;YYYY;Y;;YYYYY,Y;;YE;YYYY;YY,Y;;;Y;YYYYY;kYZYY;,YY;;Y,YY;;YYY;Y;Y;YY;;;YY'YY;YYYY;YY\n",
      "----------------------------------------\n",
      "Epoch: 4 \t Loss: 3.73232539\n",
      "----------------------------------------\n",
      "tssssssisislssssssysyisssYuyaTiYs sYlssutYWtYs;dstdyisYsysystssys;sisssysss;syuYsdYsdys;ssssssssssssssssssssssssssssssssssssssszysiszssuYsdssssnussYsystssssssssulsuyssdssssuldssYsYsusudsuysisisusul;sty\n",
      "----------------------------------------\n",
      "Epoch: 5 \t Loss: 3.65000739\n",
      "----------------------------------------\n",
      " yiiinyiddiwdwtyl sidsdl\n",
      "tITiOgd;w elniierotehnssdwtny.diyiiynoTOlsoioiytyteilenntddenittuToWsuysniOoia;lnooioyetWtoindlwwidNieddshddtod yuthniwAysylii,dsoiwOydie'duiOiyHayeiWuohentgdnedst,yinftiOlisss\n",
      "----------------------------------------\n",
      "Epoch: 6 \t Loss: 3.61020339\n",
      "----------------------------------------\n",
      "eittiiiyttaiisggiisiititdytsi?tdyyidttiytdiytydyitiinlniititiitiyidthayiiddi\n",
      "dthttAiygytnhniiyddiOtanlthsIiOtyhyinilntntisyigityyigIiitdsanh?iyittyittityAiaitihOntitiythhnidnntdnilyyotiihtyityntdtlyTOu\n",
      "----------------------------------------\n",
      "Epoch: 7 \t Loss: 3.61137414\n",
      "----------------------------------------\n",
      "ZWEoTZZZnZOWZZOZOZWZnZZZZZZnZZEl'ZWZOZZThWZZZZZTZZZZZZZtZEZZ;wNWZZTTlOyt;ZZZZnnnZgZZZteZOyW,a'ZWZi'ZwoZZlZZZ'nZZZZZPWZoZZZZZZZZENUZZZZZZZEZNlbyZZZZOZZZZKZnO,n'ZZOy'nZ'eZZtZttnZZEZEWZ.ZZ,ZZZZewZZnZNWZZE\n",
      "----------------------------------------\n",
      "Epoch: 8 \t Loss: 3.59234586\n",
      "----------------------------------------\n",
      "wylaitpeeoaATi.aSd\n",
      "n n getiOeph'etteh pno\n",
      "oiT Nnhoo aAs,eoeeoetaiAtscNBeta tt eWsseTkasytisasaglaMt\n",
      "sps;deiAhehsrW,e,toItate dtettieteNtlan paIaoiteei.eiid leeTySeiAt, titsniaiilea eeshee OI.AAetTt dOh\n",
      "----------------------------------------\n",
      "Epoch: 9 \t Loss: 3.58373189\n",
      "----------------------------------------\n",
      "EleEA ol itaitte yne,teNtilStestann is  eW h hthn ew.heasaydEe\n",
      "dne, iOeNtyeaooui,AdAtthudhnttetio,hopTeetfseOseteuuseweawotegAy.ennaneeteeN eitTkdpk Oehie uTrN EtTyey ,itiaystt lterne,t;thaTTeepesNaaOi\n",
      "----------------------------------------\n",
      "Epoch: 10 \t Loss: 3.57723428\n",
      "----------------------------------------\n",
      "i;gydnl dtselpetph,wWde;yspt,dgeggp NeitpEt\n",
      "eht iefetn iaitaenaEddipoedteyd coii.\n",
      "eyiisooyt' et'apohhtot h,oipAat,tsdWeAgi  ie iTye, iIa\n",
      "Snee\n",
      "S;I   tbetAtefeadoa?Eeie,neO i teeo insoTiniiA Ots\n",
      "eestAEa;\n",
      "----------------------------------------\n",
      "Epoch: 11 \t Loss: 3.61835201\n",
      "----------------------------------------\n",
      "tgiZgZOZ,.yZ!ZlZiio.tZiZZZxt;;ZgggbZthZlZlhggtiZtZZZtZZPZhgtgigZZZZZgiittSgiZZZZZZZoZZi;ZZZggi;ZiZZZZZZPhtZiggZittgZZ;ZZgZgiZtZgZZZhtygGiWah.ZZtZiigi;ZtKgiZZyZZZtZOgZhytZZZ;ZaZIoSZiZaiiZSt;Z;i;tZyZZgZg\n",
      "----------------------------------------\n",
      "Epoch: 12 \t Loss: 3.60104556\n",
      "----------------------------------------\n",
      "RRRR Ry i eitm  yd  emo i  et o iytiu tu   ees mu utei u   e  um i  dy  i   tu mt mi  eieeei i s iiyM usmy yu iiedu  u uIuumise e g mmtet uet emos u  imt u  uit  s  uete    r t tuide ud utt t tm uituup\n",
      "----------------------------------------\n",
      "Epoch: 13 \t Loss: 3.63135882\n",
      "----------------------------------------\n",
      "aT'sltuiitstti\n",
      "\n",
      "aiit;itysiOsiiitsinssWnyiiiisuIilsidusii.a.mT?OatWiT;yOsssOt\n",
      "idshggdgsdsiitiltiti.\n",
      "iIgsaiftslTistOiyyisasuuyiit;ysinnisdetndW:iiasisAslsiiti\n",
      "tsOdigyNsgy'iiysts;hs,'stygtisyigNaiiiyaiiti\n",
      "----------------------------------------\n",
      "Epoch: 14 \t Loss: 3.62488334\n",
      "----------------------------------------\n",
      "etee:sissssspseesssdsssess s  sssiys  sseeesses: ss:ssesssssess sssssssssssueuesssesss:ss sssssssses ssesssesssssssssss ssssdstsistsssssssesss ssesssessssusssssssssussssssesssse etss sdessessusss\n",
      " sess\n",
      "----------------------------------------\n",
      "Epoch: 15 \t Loss: 3.64678671\n",
      "----------------------------------------\n",
      "kTO;;;;;;;;;;;;;OT;;;;;;;;;;;T;;;Z;;BE;;'T;Z;;;T;;;;;;;;;;;;;;;T;;;;N;NB;T;;;;;;N;;;;;;;;;;;E;;;;;;T;;Z;;;O;;;ZZ';;Eg;;;;;T;;;;;;;T;;;;;;;;;;;;;;;;;;E;;;;;;;;;;;;;Z;;;;;B;;TEtEB;;;T;B;TB;;O;OT;Z;T;;;;Z\n",
      "----------------------------------------\n",
      "Epoch: 16 \t Loss: 3.62369454\n",
      "----------------------------------------\n",
      "thhhnroePhh,tPA,hhhiPgaha APhPDDPPPPahtAhd hPttehhqDPPnthPha-hnhDA?ilhlADht,aiPaaDP-hhWhaa,ht,aPtaraaahPlpf apa\n",
      "htthhPahatPaethaataaaeoaiala  PnhaPhhfiiahlltkPhhhAhhttPthihah'a- ha,Ph tikha,Pthah,phrhP\n",
      "----------------------------------------\n",
      "Epoch: 17 \t Loss: 3.64349650\n",
      "----------------------------------------\n",
      "\n",
      "iiiRd utu eoureiu oi udt eww oueteusiwo u yysyoWi s  tui e \n",
      "iu  utou ie  y \n",
      " otoo dteoteuuoue  u se IWWe oedwetw u   ii iu s iss  uu owsoe iw  iteseww isysuesu tgwe  u wugoeeyo ut eu:useuugldoeue iu e\n",
      "----------------------------------------\n",
      "Epoch: 18 \t Loss: 3.64169606\n",
      "----------------------------------------\n",
      "\n",
      "iiiit  u  s eis soueeooueeu u  e  u   suoheeu  eeeouo e u e uu o  eio  i it seyiuuotdw uyoeuwety   stuoa  eyuuoe syu eso   i u E   ee gtuuot se  e oe  r u sueeiBe it  u s   eeeet  t e e    t  uueeuese\n",
      "----------------------------------------\n",
      "Epoch: 19 \t Loss: 3.64886697\n",
      "----------------------------------------\n",
      "\n",
      "i eu atsseessios t\n",
      "tesd e ctssls ss  it sss\n",
      "hso seuss ssepsneseiees  siessssssost s e swis et natrs    s   oe spi ees ps it  i usesetssss s  sou  sseesss ssses siss s se s u\n",
      "seessssu\n",
      "sse s e os  es es\n",
      "----------------------------------------\n",
      "Epoch: 20 \t Loss: 3.64883446\n",
      "----------------------------------------\n",
      "\n",
      "Rsee\n",
      "eessesses eee  ssuse  ss ss    e s    Cessssse \n",
      "sss sss  t  e esess se sss  ss sesseu s s e eee  s ss s s  ssi ss ssteessse s  eese \n",
      "ts steeues ses s s:eue \n",
      " ee\n",
      "s ss  eesesse soeus ssu   e ss ese\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for i_epoch in range(1, epochs + 1):\n",
    "    # Random starting point (1st 100 chars) from data to begin\n",
    "    data_ptr = np.random.randint(100)\n",
    "    n = 0\n",
    "    running_loss = 0\n",
    "    hidden_state = rnn.initHidden()\n",
    "\n",
    "    while True:\n",
    "        input_seq = char_tensor(data[data_ptr : data_ptr + seq_len])\n",
    "        target_seq = char_tensor(data[data_ptr + 1 : data_ptr + seq_len + 1])\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target_seq = target_seq.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_seq)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute gradients and take optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Detach hidden state to prevent backprop through entire training history\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "        # Update the data pointer\n",
    "        data_ptr += seq_len\n",
    "        n += 1\n",
    "\n",
    "        # If at end of data, break\n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "\n",
    "    # Print loss and save weights after every epoch\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss / n))\n",
    "    epochs_cnt.append(i_epoch)\n",
    "    losses.append(running_loss / n);\n",
    "    torch.save(rnn.state_dict(), save_path)\n",
    "\n",
    "    # Sample / generate a text sequence after every epoch\n",
    "    data_ptr = 0\n",
    "    hidden_state = rnn.initHidden()\n",
    "\n",
    "    # Random character from data to begin\n",
    "    rand_index = np.random.randint(data_size - 1)\n",
    "    input_seq = char_tensor(data[rand_index : rand_index + 1])\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "        # Construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "\n",
    "        # Print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "\n",
    "        # Next input is current output\n",
    "        input_seq = char_tensor(ix_to_char[index.item()])\n",
    "        data_ptr += 1\n",
    "\n",
    "        if data_ptr > op_seq_len:\n",
    "            break\n",
    "\n",
    "    print(\"\\n----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e67b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(epochs_cnt, losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53938700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "Epoch: 1 \t Loss: 13.76079744\n",
      "----------------------------------------\n",
      "e sleee whinge rasle rt'tre rasletre ingee rasleeee ralee rasleeee thon; sle inge re sle sle rasleeee sle maslee re inge thyee uasleee sle olet sle rasleeeee sle occcccccccccccccccccccccccccccccccccccc\n",
      "----------------------------------------\n",
      "Epoch: 2 \t Loss: 16.47507999\n",
      "----------------------------------------\n",
      " sle, uandsle, tee, t tre, tr tr uandsle, tr tr ouandsle, th'sle, tr t aeb\n",
      "N:\n",
      "ON:\n",
      "NIAN:\n",
      "Th'sle sle, tr tr sle, tee, te, tee, spe, tre, tr thyeble, tr uandseble, t, windsleee, seble, tar tr t uandsle, t\n",
      "----------------------------------------\n",
      "Epoch: 3 \t Loss: 17.69790749\n",
      "----------------------------------------\n",
      "SEBu rebandeban.\n",
      "\n",
      "SEBu lebandeban.\n",
      "\n",
      "SEBu deban.\n",
      "\n",
      "SEBu deban.\n",
      "\n",
      "SEBu deblebandeban.\n",
      "\n",
      "SEBur deeeban.\n",
      "\n",
      "SEBu deblebandebandebandebandeban.\n",
      "\n",
      "SEBu deban.\n",
      "\n",
      "SEBu deblebaneban.\n",
      "\n",
      "SEBu debaneban.\n",
      "\n",
      "SEBu deban.\n",
      "\n",
      "SEB\n",
      "----------------------------------------\n",
      "Epoch: 4 \t Loss: 18.22739432\n",
      "----------------------------------------\n",
      "SThidindindidindindindindindindindindindindindindindindindindindindindindindindindindidindist bo SThe, let SThidist SThing, STho STho SThidindindindindindindindindindindindindidindidindindindindindidin\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105/1792571264.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_105/1792571264.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, hidden_state)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use clone to avoid in-place operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m  \u001b[0;31m# Add residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    880\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Data I/O\n",
    "data = open('input.txt', 'r').read()  # This should be a simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 100  # Size of hidden layer of neurons\n",
    "seq_len = 25  # Number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "epochs = 20\n",
    "save_path = 'model.pth'\n",
    "op_seq_len = 200  # Length of the output sequence\n",
    "num_layers = 1\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # Change embedding size to match hidden size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        residual = embedding.clone()  # Use clone to avoid in-place operations\n",
    "        output, hidden_state = self.lstm(embedding, hidden_state)\n",
    "        output = output + residual  # Add residual connection\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_size).to(device))\n",
    "\n",
    "# Instantiate model, loss function and optimizer\n",
    "rnn = ResidualLSTM(vocab_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert a sequence of characters to a tensor\n",
    "def char_tensor(seq):\n",
    "    tensor = torch.tensor([char_to_ix[ch] for ch in seq], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return tensor\n",
    "\n",
    "losses = []\n",
    "epochs_cnt = []\n",
    "\n",
    "# Training loop\n",
    "for i_epoch in range(1, epochs + 1):\n",
    "    # Random starting point (1st 100 chars) from data to begin\n",
    "    data_ptr = np.random.randint(100)\n",
    "    n = 0\n",
    "    running_loss = 0\n",
    "    hidden_state = rnn.initHidden()\n",
    "\n",
    "    while True:\n",
    "        input_seq = char_tensor(data[data_ptr : data_ptr + seq_len])\n",
    "        target_seq = char_tensor(data[data_ptr + 1 : data_ptr + seq_len + 1])\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target_seq = target_seq.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_seq)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute gradients and take optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Detach hidden state to prevent backprop through entire training history\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "        # Update the data pointer\n",
    "        data_ptr += seq_len\n",
    "        n += 1\n",
    "\n",
    "        # If at end of data, break\n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "\n",
    "    # Print loss and save weights after every epoch\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss / n))\n",
    "    epochs_cnt.append(i_epoch)\n",
    "    losses.append(running_loss / n)\n",
    "    torch.save(rnn.state_dict(), save_path)\n",
    "\n",
    "    # Sample / generate a text sequence after every epoch\n",
    "    data_ptr = 0\n",
    "    hidden_state = rnn.initHidden()\n",
    "\n",
    "    # Random character from data to begin\n",
    "    rand_index = np.random.randint(data_size - 1)\n",
    "    input_seq = char_tensor(data[rand_index : rand_index + 1])\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "        # Construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "\n",
    "        # Print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "\n",
    "        # Next input is current output\n",
    "        input_seq = char_tensor(ix_to_char[index.item()])\n",
    "        data_ptr += 1\n",
    "\n",
    "        if data_ptr > op_seq_len:\n",
    "            break\n",
    "\n",
    "    print(\"\\n----------------------------------------\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(epochs_cnt, losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b04c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
